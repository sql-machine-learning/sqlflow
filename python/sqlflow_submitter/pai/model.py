# Copyright 2020 The SQLFlow Authors. All rights reserved.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import io
import os
import pickle
import tarfile

import odps
import tensorflow as tf
from sqlflow_submitter import db
from tensorflow.python.platform import gfile


def get_oss_path_from_uri(oss_model_dir, file_name):
    uri_parts = oss_model_dir.split("?")
    if len(uri_parts) != 2:
        raise ValueError("error oss_model_dir: ", oss_model_dir)
    oss_path = "/".join([uri_parts[0].rstrip("/"), file_name])
    return oss_path


def save_dir(oss_model_dir, local_dir):
    oss_dir = oss_model_dir.split("?")[0]
    for (root, dirs, files) in os.walk(local_dir, topdown=True):
        dst_dir = "/".join([oss_dir, root])
        gfile.MakeDirs(dst_dir)
        for file_name in files:
            curr_file_path = os.path.join(root, file_name)
            remote_file_path = "/".join([dst_dir, file_name])
            gfile.Copy(curr_file_path, remote_file_path)
    # oss://bucket/path/to/checkpoint   <-   my_model/exported_path


def save_file(oss_model_dir, file_name):
    '''
    Save the local file to OSS direcotory using GFile.
    '''
    print("creating oss dirs: %s" % oss_model_dir)
    oss_path = get_oss_path_from_uri(oss_model_dir, file_name)
    oss_dir = oss_model_dir.split("?")[0]
    print("creating oss dirs: %s" % oss_dir)
    gfile.MakeDirs(oss_dir)
    fn = open(file_name, "r")
    with gfile.GFile(oss_path, mode='w') as f:
        while True:
            buf = fn.read(4096)
            if not buf:
                break
            f.write(buf)
    fn.close()


def load_file(oss_model_dir, file_name):
    '''
    Load file from OSS to local directory.
    '''
    oss_path = get_oss_path_from_uri(oss_model_dir, file_name)
    fn = open(file_name, "w")
    reader = gfile.GFile(oss_path, mode="r")
    while True:
        buf = reader.read(4096)
        if not buf:
            break
        fn.write(buf)
    reader.close()
    fn.close()


def save_metas(oss_model_dir, num_workers, file_name, *meta):
    '''
    Save model descriptions like the training SQL statements to OSS directory.
    Data are saved using pickle.
    Args:
        oss_model_dir: OSS URI that the model will be saved to.
        *meta: python objects to be saved.
    Return:
        None
    '''
    if num_workers > 1:
        FLAGS = tf.app.flags.FLAGS
        if FLAGS.task_index != 0:
            print("skip saving model desc on workers other than worker 0")
            return
    oss_path = get_oss_path_from_uri(oss_model_dir, file_name)
    with gfile.GFile(oss_path, mode='w') as f:
        pickle.dump(list(meta), f)
    # write a file "file_name_estimator" to store the estimator name, so we
    # can determine if the estimator is BoostedTrees* when explaining the model.
    oss_path = get_oss_path_from_uri(oss_model_dir,
                                     "_".join([file_name, "estimator"]))
    with gfile.GFile(oss_path, mode='w') as f:
        f.write(meta[0])


def load_metas(oss_model_dir, file_name):
    '''
    Load and restore a directory and metadata that are saved by `model.save`
    from a MaxCompute table
    Args:
        oss_model_dir: OSS URI that the model will be saved to.
    Return:
        A list contains the saved python objects
    '''
    uri_parts = oss_model_dir.split("?")
    if len(uri_parts) != 2:
        raise ValueError("error oss_model_dir: ", oss_model_dir)
    oss_path = "/".join([uri_parts[0].rstrip("/"), file_name])

    reader = gfile.GFile(oss_path, mode='r')
    return pickle.load(reader)
