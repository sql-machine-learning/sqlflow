# Copyright 2020 The SQLFlow Authors. All rights reserved.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""This module saves or loads the SQLFlow model.
"""
import os
import tempfile
from enum import Enum

from runtime.model import oss
from runtime.model.db import read_with_generator, write_with_generator
from runtime.model.tar import unzip_dir, zip_dir

try:
    import cPickle as pickle
except ModuleNotFoundError:
    import pickle

# archive the current work director into a tarball
TARBALL_NAME = "model.tar.gz"

# serialize the Model object into file
MODEL_OBJ_FILE_NAME = "sqlflow_model.pkl"


class EstimatorType(Enum):
    """The enum type for various SQLFlow estimator.
    """
    # To stay compitable with old models, we start at 0
    TENSORFLOW = 0
    XGBOOST = 1
    # PAIML is the model type that trained by PAI machine learning algorithm
    # toolkit
    PAIML = 2


class Model(object):
    """Model module represents a SQLFlow trained model, which includes
    three parts:
    1. the estimator type indicates which SQLFlow estimator comes from.
    2. the model meta indicates the meta data of training .e.g attributions,
    feature column types.
    3. the model data indicated the trained model, which generated by the AI
    engine, .e.g TensorFlow, XGBoost.

    Usage:

        meta = runtime.model.collect_metadata(attributes={...}, ...)
        m = runtime.model.Model(ModelType.XGBOOST, meta)
        m.save(datasource="mysql://", "sqlflow_models.my_model")

    """
    def __init__(self, typ, meta):
        """
        Args:
            typ: EstimatorType
                the enum value of EstimatorType.
            meta: JSON
                the training meta with JSON format.
        """
        self._typ = typ
        self._meta = meta

    def _zip(self, local_dir, tarball):
        """
        Zip the model information and all files in local_dir into a tarball.

        Args:
            local_dir (str): the local directory.
            tarball (str): the tarball path.

        Returns:
            None.
        """
        model_obj_file = os.path.join(local_dir, MODEL_OBJ_FILE_NAME)
        _dump_pkl(self, model_obj_file)
        zip_dir(local_dir, tarball, arcname="./")
        os.remove(model_obj_file)

    @staticmethod
    def _unzip(local_dir, tarball):
        """
        Unzip the tarball into local_dir and deserialize the model
        information.

        Args:
            local_dir (str): the local directory.
            tarball (str): the tarball path.

        Returns:
            Model: a Model object represent the model type and meta
            information.
        """
        model_obj_file = os.path.join(local_dir, MODEL_OBJ_FILE_NAME)
        unzip_dir(tarball, local_dir)
        model = _load_pkl(model_obj_file)
        os.remove(model_obj_file)
        return model

    def save_to_db(self, datasource, table, local_dir=os.getcwd()):
        """
        This save function would archive all the files on local_dir
        into a tarball, and save it into DBMS with the specified table
        name.

        Args:
            datasource (str): the connection string to DBMS.
            table (str): the saved table name.
            local_dir (str): the local directory to save.

        Returns:
            None.
        """
        with tempfile.TemporaryDirectory() as tmp_dir:
            tarball = os.path.join(tmp_dir, TARBALL_NAME)
            self._zip(local_dir, tarball)

            def _bytes_reader(filename, buf_size=8 * 32):
                def _gen():
                    with open(filename, "rb") as f:
                        while True:
                            data = f.read(buf_size)
                            if data:
                                yield data
                            else:
                                break

                return _gen

            write_with_generator(datasource, table, _bytes_reader(tarball))

    @staticmethod
    def load_from_db(datasource, table, local_dir=os.getcwd()):
        """
        Load the saved model from DBMS and unzip it on local_dir.

        Args:
            datasource (str): the connection string to DBMS
            table (str): the table name which saved in DBMS
            local_dir (str): the local directory to load.

        Returns:
            Model: a Model object represent the model type and meta
            information.
        """
        with tempfile.TemporaryDirectory() as tmp_dir:
            tarball = os.path.join(tmp_dir, TARBALL_NAME)
            gen = read_with_generator(datasource, table)
            with open(tarball, "wb") as f:
                for data in gen():
                    f.write(bytes(data))

            return Model._unzip(local_dir, tarball)

    def save_to_oss(self, oss_model_dir, local_dir=os.getcwd()):
        """
        This save function would archive all the files on local_dir
        into a tarball, and save it into OSS model directory.

        Args:
            oss_model_dir (str): the OSS model directory to save.
                It is in the format of oss://bucket/path/to/dir/.
            local_dir (str): the local directory to save.

        Returns:
            None.
        """
        with tempfile.TemporaryDirectory() as tmp_dir:
            tarball = os.path.join(tmp_dir, TARBALL_NAME)
            self._zip(local_dir, tarball)
            oss.save_file(oss_model_dir, tarball, TARBALL_NAME)

    @staticmethod
    def load_from_oss(oss_model_dir, local_dir=os.getcwd()):
        """
        Load the saved model from OSS and unzip it on local_dir.

        Args:
            oss_model_dir (str): the OSS model directory to load.
                It is in the format of oss://bucket/path/to/dir/.
            local_dir (str): the local directory to load.

        Returns:
            Model: a Model object represent the model type and meta
            information.
        """
        with tempfile.TemporaryDirectory() as tmp_dir:
            tarball = os.path.join(tmp_dir, TARBALL_NAME)
            oss.load_file(oss_model_dir, tarball, TARBALL_NAME)
            return Model._unzip(local_dir, tarball)


def _dump_pkl(obj, to_file):
    """Dump the Python object to file with Pickle.
    """
    with open(to_file, "wb") as f:
        pickle.dump(obj, f, protocol=2)


def _load_pkl(from_file):
    """Load the Python object from a file with Pickle.
    """
    with open(from_file, "rb") as f:
        return pickle.load(f)
