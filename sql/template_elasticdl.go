// Copyright 2019 The SQLFlow Authors. All rights reserved.
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package sql

const elasticdlDataConversionTemplateText = `
import os

from elasticdl.python.common.constants import ODPSConfig
from elasticdl.python.common.odps_io import ODPSReader
from elasticdl.python.common.odps_recordio_conversion_utils import (
    write_recordio_shards_from_iterator,
)


COLUMN_NAMES = {{.FeaturesList}}

reader = ODPSReader(
    os.environ[ODPSConfig.PROJECT_NAME],
    os.environ[ODPSConfig.ACCESS_ID],
    os.environ[ODPSConfig.ACCESS_KEY],
    os.environ[ODPSConfig.ENDPOINT],
    table = "{{.ODPSTableName}}",
    partition = None,
    num_processes = {{.NumProcesses}},
)

records_iter = reader.to_iterator(
    num_workers = 1,
    worker_index = 0,
    batch_size = {{.BatchSize}},
    epoch = 1,
    shuffle = False,
    columns = COLUMN_NAMES,
)

write_recordio_shards_from_iterator(
    records_iter,
    COLUMN_NAMES,
    output_dir = "{{.RecordIODataDir}}",
    records_per_shard = 200,
)
`

// TODO: Right now this is just copy of the example code. Need to fill in
// values generated by filler later.
const elasticdlTrainTemplateText = `
import os

import tensorflow as tf

from elasticdl.python.common.constants import Mode, ODPSConfig
from elasticdl.python.common.log_util import default_logger as logger
from elasticdl.python.common.odps_io import ODPSWriter
from elasticdl.python.worker.prediction_outputs_processor import (
    BasePredictionOutputsProcessor,
)


def custom_model():
    inputs = tf.keras.layers.Input(shape=(32, 32, 3), name="image")
    use_bias = True

    conv = tf.keras.layers.Conv2D(
        32,
        kernel_size=(3, 3),
        padding="same",
        use_bias=use_bias,
        activation=None,
    )(inputs)
    bn = tf.keras.layers.BatchNormalization(
        epsilon=1e-06, axis=-1, momentum=0.9
    )(conv)
    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)

    conv = tf.keras.layers.Conv2D(
        32,
        kernel_size=(3, 3),
        padding="same",
        use_bias=use_bias,
        activation=None,
    )(activation)
    bn = tf.keras.layers.BatchNormalization(
        epsilon=1e-06, axis=-1, momentum=0.9
    )(conv)
    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)

    max_pool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(activation)
    dropout = tf.keras.layers.Dropout(0.2)(max_pool)

    conv = tf.keras.layers.Conv2D(
        64,
        kernel_size=(3, 3),
        padding="same",
        use_bias=use_bias,
        activation=None,
    )(dropout)
    bn = tf.keras.layers.BatchNormalization(
        epsilon=1e-06, axis=-1, momentum=0.9
    )(conv)
    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)

    conv = tf.keras.layers.Conv2D(
        64,
        kernel_size=(3, 3),
        padding="same",
        use_bias=use_bias,
        activation=None,
    )(activation)
    bn = tf.keras.layers.BatchNormalization(
        epsilon=1e-06, axis=-1, momentum=0.9
    )(conv)
    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)

    max_pool = tf.keras.layers.MaxPooling2D()(activation)
    dropout = tf.keras.layers.Dropout(0.3)(max_pool)

    conv = tf.keras.layers.Conv2D(
        128,
        kernel_size=(3, 3),
        padding="same",
        use_bias=use_bias,
        activation=None,
    )(dropout)
    bn = tf.keras.layers.BatchNormalization(
        epsilon=1e-06, axis=-1, momentum=0.9
    )(conv)
    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)

    conv = tf.keras.layers.Conv2D(
        128,
        kernel_size=(3, 3),
        padding="same",
        use_bias=use_bias,
        activation=None,
    )(activation)
    bn = tf.keras.layers.BatchNormalization(
        epsilon=1e-06, axis=-1, momentum=0.9
    )(conv)
    activation = tf.keras.layers.Activation(tf.nn.relu)(bn)

    max_pool = tf.keras.layers.MaxPooling2D()(activation)
    dropout = tf.keras.layers.Dropout(0.4)(max_pool)

    flatten = tf.keras.layers.Flatten()(dropout)
    outputs = tf.keras.layers.Dense({{.PredictOutputShape}}, name="output")(flatten)

    return tf.keras.Model(inputs=inputs, outputs=outputs, name="cifar10_model")


def loss(output, labels):
    labels = tf.reshape(labels, [-1])
    return tf.reduce_mean(
        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=output, labels=labels
        )
    )


def optimizer(lr=0.1):
    return tf.optimizers.SGD(lr)


def dataset_fn(dataset, mode):
    def _parse_data(record):
        if mode == Mode.PREDICTION:
            feature_description = {
                {{.FeaturesDescription}}
            }
        else:
            feature_description = {
                {{.FeaturesDescription}}
                {{if .IsTraining}}
                "{{.LabelColName}}": tf.io.FixedLenFeature([1], tf.int64),
                {{end}}
            }
        parsed_example = tf.io.parse_single_example(record, feature_description)

        if mode == Mode.PREDICTION:
            return parsed_example
        {{if .IsTraining}}
        else:
            del parsed_example["{{.LabelColName}}"]
            return parsed_example, tf.cast(parsed_example["{{.LabelColName}}"], tf.int32)
        {{end}}

    dataset = dataset.map(_parse_data)

    {{if .IsTraining}}
    if mode != Mode.PREDICTION and "{{.TrainClause.EnableShuffle}}" == "true":
        dataset = dataset.shuffle(buffer_size={{.TrainClause.ShuffleBufferSize}})
    {{end}}

    return dataset


def eval_metrics_fn(predictions, labels):
    labels = tf.reshape(labels, [-1])
    return {
        "accuracy": tf.reduce_mean(
            input_tensor=tf.cast(
                tf.equal(
                    tf.argmax(predictions, 1, output_type=tf.dtypes.int32),
                    labels,
                ),
                tf.float32,
            )
        )
    }


class PredictionOutputsProcessor(BasePredictionOutputsProcessor):
    def __init__(self):
        if all(
            k in os.environ
            for k in (
                ODPSConfig.PROJECT_NAME,
                ODPSConfig.ACCESS_ID,
                ODPSConfig.ACCESS_KEY,
                ODPSConfig.ENDPOINT,
            )
        ):
            self.odps_writer = ODPSWriter(
                os.environ[ODPSConfig.PROJECT_NAME],
                os.environ[ODPSConfig.ACCESS_ID],
                os.environ[ODPSConfig.ACCESS_KEY],
                os.environ[ODPSConfig.ENDPOINT],
                table = "{{.PredictOutputTable}}",
                columns=["pred_" + str(i) for i in range({{.PredictOutputShape}})],
                column_types=["double" for _ in range({{.PredictOutputShape}})],
            )
        else:
            self.odps_writer = None

    def process(self, predictions, worker_id):
        if self.odps_writer:
            self.odps_writer.from_iterator(
                iter(predictions.numpy().tolist()), worker_id
            )
        else:
            logger.info(predictions.numpy())
`
