# SQLFlow Trains Models via XGBoost in Katib

## Requirement description:

SQLFlow needs to provide users with a simple way to train their ML model. This function is supported by SQLFlow running XGBoost jobs on Katib.

## Hyperparameter Optimization in Katib

[Katib](https://github.com/kubeflow/katib) is a Kubernetes Native System for Hyperparameter Tuning and Neural Architecture Search. The system is inspired by Google vizier and supports multiple ML/DL frameworks (e.g. TensorFlow, MXNet, and PyTorch).

## Support XGBoost job in Katib

[XGBoost](https://xgboost.readthedocs.io/en/latest/) is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. Now XGBoost is not supported on Katib. 

When to run XGBoost jobs in Katib, we create an Experiment CR on Kubernetes via a yaml file. This Experiment CR will create one Suggestion CR and multiple Trail Pods later. The Suggestion CR generates the value for hyperparameters. Each Trail Pod include two containers: a job container and a MetricsCollector container. The job container is created from a standard XGBoost Docker image created by us. When the job container is started, it receives paramters defined in SQLFlow as well as value of hyperparameters from Suggestion CR.

MetricsCollector container parse logs of the job containers and put training results into Katib data store. When the training job is complete, SQLFlow can read results from Katib data store.

## Pipeline:

1. Users input SQL queries. 
2. Based on input SQL queries, the codegen `codegen_katib_xgboost.go` generates `katib_xgboost.py` file. This file include all parameters for XGBoost jobs. 
3. SQLFlow server executes `katib_xgboost.py`: This python program:
   1. generates `train_xgboost.py` file. All input parameters will be filled in `train_xgboost.py`.
   2. gets all input data files (e.g., `train_data.txt`).
   3. generates a Dockerfile and build a XGBoost docker image from this Dockerfile. The `train_xgboost.py` and required data file will be copied into this image.
   4. pushes this docker image into docker.io repository.
   5. generates `katib_xgboost.yaml` file and fill it with: (1) source of docker image generated above; and (2) commands to execute `train_xgboost.py` in the container.
   5. to submit and execute `katib_xgboost.yaml` on kubernetes.
3. Katib creates an experiments to run XGboost job:
   1. Kubernetes create a Suggestion which generates value of hyperparameters;
   2. Kubernetes create multiple Trial Pods to execute xgboost traning job. Each Trial job includes two containers: (1) one container reads hyperparameters value generated by Suggestion and executes xgboost job; (2) a MetricsCollector collect and parse logs from the other container.
   3. The MetricsCollector write data into the store of Katib.
   

